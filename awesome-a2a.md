## Multimodal Models

### PandaGPT
A unified multimodal instruction-following model that combines ImageBind encoders with Vicuna LLM to enable seamless understanding across multiple modalities (images, video, audio, text, depth, thermal, and IMU). Notable for its ability to perform zero-shot cross-modal operations and multimodal arithmetic without specific training for each modality combination. The model demonstrates emergent capabilities in complex tasks like video-based storytelling and cross-modal reasoning, while only requiring aligned image-text pairs for training.

**Key Features:**
- Zero-shot cross-modal understanding
- Multimodal arithmetic operations
- Complex task handling (QA, creative writing, reasoning)
- Built on ImageBind + Vicuna architecture

**Technical Details:**
- Architecture: ImageBind encoders + Vicuna LLM
- Training: Aligned image-text pairs only
- Modalities: Images, video, audio, text, depth, thermal, IMU
- Zero-shot capabilities across modality combinations

**Reference:** [Paper link]
**Code:** [Repository link when available]
